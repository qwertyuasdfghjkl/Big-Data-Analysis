---
title: "HW2_Liu Zi Jian"
author: "Zi Jian Liu"
date: "05/02/2021"
output:
  pdf_document:
    toc: yes
    keep_tex: yes
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1  
## 1 a)
```{r}
housingData <- read.csv('_data_hw2/housingprice.csv')
head(housingData)
# coverting zipcode column into factors
housingData$zipcode <- as.factor(housingData$zipcode)
```


Finding the most expensive zipcodes:  
```{r}
avgPrice <- tapply(housingData$price, housingData$zipcode, mean)
avgPrice <- sort(avgPrice, decreasing = TRUE)
avgPrice[1:3]
```
The top 3 zipcodes whose housing prices are the most expensive is 98039, 98004, and 98040  

Boxplots of housing prices for these 3 zipcodes:
```{r}
plot1 = subset(housingData, zipcode == 98039, select = c(price))
plot2 = subset(housingData, zipcode == 98004, select = c(price))
plot3 = subset(housingData, zipcode == 98040, select = c(price))

boxplot(plot1)
boxplot(plot2)
boxplot(plot3)
```
  
Above is the boxplots of the housing prices for the most expensive 3 zipcodes.

## 1 b). Scatter plot  
```{r}
plot(housingData$sqft_living, housingData$price, main="relationship between sqft and price",
   xlab="sqft living", ylab="price", pch=20)
```
  
above is a scatterplot showing the relationship between sqft and price.

## 1 c).

```{r}
train = read.csv('_data_hw2/train.data.csv')
head(train)

test = read.csv('_data_hw2/test.data.csv')
head(test)
```
```{r}
# training
modelprice = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot, data = train)
summary(modelprice)
```
The Rsquared of the model on training data is 0.5101

```{r}
# Evaluate model on test data
testpredict = predict(modelprice, test)
prices = test$price
e = prices - testpredict
R2 = 1-sum(e^2)/sum((prices-mean(prices))^2)
R2
```
The Rsquared of the model on testing data is 0.505  

## 1. d) adding zipcode  
```{r}
# training
modelpricezip = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot +zipcode, data = train)
summary(modelpricezip)
```
The Rsquared of the model on training data is 0.5163

```{r}
# Evaluate model on test data
testpredictzip = predict(modelpricezip, test)
prices = test$price
e = prices - testpredictzip
R2zip = 1-sum(e^2)/sum((prices-mean(prices))^2)
R2zip
```
The Rsquared of the model on testing data is 0.512  
  
## 1. e)  
```{r}
fancy = read.csv('_data_hw2/fancyhouse.csv')
head(fancy)
```
```{r}
billgate = predict(modelpricezip, fancy)
billgate
```
We predict that the price of bill gates house is 15,642,273. This is a reasonable estimate for the price.  

  
## 1. f)  
```{r}
n = nrow(train)
d = length(train)
n
d
```
if n>d+1 (15129 > 22 + 1) then adding another covariate never hurts R2 over the training data.

```{r}
covariate4 = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot, data = train)
covariate5 = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + zipcode, data = train)
covariate6 = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + zipcode + floors, data = train)
covariate7 = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + zipcode + floors + condition, data = train)
covariate8 = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + zipcode + floors + condition + grade, data = train)
summary(covariate4)
summary(covariate5)
summary(covariate6)
summary(covariate7)
summary(covariate8)
```
As we can see above, adding another covariate to our model either does not change our Rsquared or increases our Rsquared.

# Question 2.  

## 2 a).  
```{r}
modelmult = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + zipcode + bedrooms * bathrooms, data = train)
summary(modelmult)
```
The R2 of the new model on the training data is 0.5224
```{r}
newmodelpred = predict(modelmult, test)
prices = test$price
e = prices - newmodelpred
R2 = 1-sum(e^2)/sum((prices-mean(prices))^2)
R2
```
The R squared of the new model on the testing data is 0.5165.  

## 2 b).  
Another feature engineering that further improves the model that we have in question 2. a) is that we can try transformations of the original features, such as trying log, sqrt, squared of different features to improve the Rsquared of the testing data.
  
```{r}
modelsquare = lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + sqrt(zipcode) + bedrooms * bathrooms, data = train)
summary(modelsquare)
newmodelpred2 = predict(modelsquare, test)
prices = test$price
e = prices - newmodelpred2
R2 = 1-sum(e^2)/sum((prices-mean(prices))^2)
R2
```
If we input the sqrt of zipcode instead of zipcode in our model, the Rsquared of our testing data increases from 0.5165114 to 0.516512. 

## 2 c).  
```{r}
modelpoly = lm(price ~ poly(bedrooms, 2) + poly(bathrooms, 3) + sqft_living + sqft_lot + zipcode, data = train)
summary(modelpoly)
```
```{r}
modelpolypred = predict(modelpoly, test)
prices = test$price
e = prices - modelpolypred
R2 = 1-sum(e^2)/sum((prices-mean(prices))^2)
R2
```
By using a polynomial term of bedrooms and bathrooms variables of degrees 2 and 3, we find that the Rsquared of the new model on training data is = 0.5423 and on testing data = 0.5285. This is an increase of Rsquared as compared to before.


# Question 3. Wine Pricing  
## 3 Part I. Preliminary Analysis  

```{r}
wineData <- read.csv('_data_hw2/wine.csv')
head(wineData)
```
```{r}
plot(wineData$AGST, wineData$Price, main="relationship between AGST and price",
   xlab="average growing season temperature (AGST)", ylab="price", pch=20)
plot(wineData$WinterRain, wineData$Price, main="relationship between winterRain and price",
   xlab="winter rain amount", ylab="price", pch=20)
plot(wineData$HarvestRain, wineData$Price, main="relationship between harvest rain and price",
   xlab="Harvest rain amount", ylab="price", pch=20)
plot(wineData$Age, wineData$Price, main="relationship between age and price",
   xlab="age of wine", ylab="price", pch=20)
```
  
The variable that looks to be the most correlated with Price would be AGST. The scatter plot looks the most patterned and we see that generally, the price increases as AGST increases, an upwards trend.
```{r}
AGST <- cor.test(wineData$AGST, wineData$Price, 
                    method = "pearson")
winterRain <- cor.test(wineData$WinterRain, wineData$Price, 
                    method = "pearson")
harvestRain <- cor.test(wineData$HarvestRain, wineData$Price, 
                    method = "pearson")
ageWine <- cor.test(wineData$Age, wineData$Price, 
                    method = "pearson")
AGST
winterRain
harvestRain
ageWine
```
  
As we can see from the pearson's correlation scores above, the cor of 0.6596 for AGST is the highest correlation between a variable and price. This backs up our claim before that AGST is the most correlated with price.  

## 3. Part II. Marginal Regression Analysis  
```{r}

# install.packages("rlang")
library(rlang)
```

```{r}
#install.packages('ggplot2')
library(ggplot2)
#install.packages('sjPlot')
library(sjPlot)
```

```{r}
fit <- lm(Price ~ AGST, data = wineData)
plot_model(fit, type = "pred")
summary(fit)
```
The fitted coefficient values is -3.4178 and 0.6351. The intercept is -3.4178 and the slope is 0.6351. For every 1 unit increase in AGST, price increases by 0.6351. The Rsquared value is 0.435.

## 3. Part III. Multiple Regression Analysis
```{r}
multi_fit2 = lm(Price ~ AGST + HarvestRain, data = wineData)
summary(multi_fit2)
multi_fit3 = lm(Price ~ AGST + HarvestRain + Age, data = wineData)
summary(multi_fit3)
multi_fit4 = lm(Price ~ AGST + HarvestRain + Age + WinterRain, data = wineData)
summary(multi_fit4)
multi_fit5 = lm(Price ~ AGST + HarvestRain + Age + WinterRain + FrancePop, data = wineData)
summary(multi_fit5)
```
As we add more covriates, we find that R squared on the training data increases. Which generally could mean that the model is fitted better. We find that at 1 variable, Rsquared = 0.435, increasing to 0.7074, 0.79, 0.8286, and finally 0.8294 at 5 covariates. The model that we choose based on Rsquared is Price ~ AGST + HarvestRain + Age + WinterRain + FrancePop since the Rsquared is the highest. Note: sometimes having a high Rsquared does not lead to a better model.

```{r}
winetestData <- read.csv('_data_hw2/winetest.csv')
head(winetestData)
```
```{r}
testfit <- lm(Price ~ AGST, data = winetestData)
summary(testfit)
testmulti_fit2 = lm(Price ~ AGST + HarvestRain, data = winetestData)
summary(testmulti_fit2)
testmulti_fit3 = lm(Price ~ AGST + HarvestRain + Age, data = winetestData)
summary(testmulti_fit3)
testmulti_fit4 = lm(Price ~ AGST + HarvestRain + Age + WinterRain, data = winetestData)
summary(testmulti_fit4)
testmulti_fit5 = lm(Price ~ AGST + HarvestRain + Age + WinterRain + FrancePop, data = winetestData)
summary(testmulti_fit5)
```
From the winetest data provided, since all of the variables is collinear to AGST, adding covariates does not change the Rsquared and the coefficient values. Also there is only 2 data entries. The Rsquared value for each model is equal to 1. The model that we choose based on Rsquared is Price ~ AGST since all of the variables are collinear.
```{r}
multi_fit5 = lm(Price ~ AGST + HarvestRain + Age + WinterRain + FrancePop, data = wineData)
summary(multi_fit5)
```
As seen above, we can see that rains in the winter followed by hot summer (high AGST) both has a positive relation with price. However the hot summer affects the price of the wine a lot more than the rain in the winter. Rainfall at harvest also has a negative relation with price. If we take that the price reflects the quality of wine, then Prof. Ashenfelter's findings is consistent with our model.

# Question 4.  Moneyball  
# 4. Part I. Preliminary Analysis  
```{r}
baseballData <- read.csv('_data_hw2/baseball.csv')
head(baseballData)
```

```{r}
hist(baseballData$OBP)
boxplot(baseballData$OBP)
mean(baseballData$OBP)
median(baseballData$OBP)
```
Histogram and boxplot for OBP. The mean and median of OBP is 0.3263312 and 0.326 respectively. This means that the distribution is not skewed. You can also verify this from the boxplot and histogram.

```{r}
hist(baseballData$SLG)
boxplot(baseballData$SLG)
mean(baseballData$SLG)
median(baseballData$SLG)
```
Histogram and boxplot for SLG. The mean and median of OBP is 0.3973417 and 0.396 respectively. This means that the distribution is not skewed. You can also verify this from the boxplot and histogram.

```{r}
hist(baseballData$BA)
boxplot(baseballData$BA)
mean(baseballData$BA)
median(baseballData$BA)
```
Histogram and boxplot for BA. The mean and median of OBP is 0.2592727 and 0.26 respectively. This means that the distribution is not skewed. You can also verify this from the boxplot and histogram.

## 4. Part II. Marginal Regression Analysis
```{r}
BAfit <- lm(RS ~ BA, data = baseballData)
summary(BAfit)
plot(baseballData$BA, baseballData$RS)
abline(BAfit)
```
  
Above is the scatter plot using model RS ~ BA. The intercept and slope are -805.51, and 5864.84 respectively, the Rsquared is 0.6839
```{r}
plot(BAfit)
```
  
From the second plot, the QQ plot of the fitted residuals for model RS ~ BA, we can verify that the residuals are not skewly distributed and that the model is reasonable.

```{r}
OBPfit <- lm(RS ~ OBP, data = baseballData)
summary(OBPfit)
plot(baseballData$OBP, baseballData$RS)
abline(OBPfit)
```
  
Above is the scatter plot using model RS ~ OBP. The intercept and slope are -1076.6, and 5490.4 respectively, the Rsquared is 0.8109
```{r}
plot(OBPfit)
```
  
From the second plot, the QQ plot of the fitted residuals for model RS ~ OBP, we can verify that the residuals are not skewly distributed and that the model is reasonable.

```{r}
SLGfit <- lm(RS ~ SLG, data = baseballData)
summary(SLGfit)
plot(baseballData$SLG, baseballData$RS)
abline(SLGfit)
```
  
Above is the scatter plot using model RS ~ SLG. The intercept and slope are -289.37, and 2527.92 respectively, the Rsquared is 0.8441
```{r}
plot(SLGfit)
```
  
From the second plot, the QQ plot of the fitted residuals for model RS ~ SLG, we can verify that the residuals are not skewly distributed and that the model is reasonable.

Comparing the Rsquared results, we see that Rsquared for BA = 0.6839, Rsquared for OBP = 0.8109 and Rsquared for SLG = 0.8441. We see that the Rsquared result for OBP and for SLG is higher than the Rsquared for BA. That is consistent with Billy's claim that OBP and SLG have much more impact than BA. This is not consistent with the intuition that BA is thought to be the most responsible for RS.  

## 4. Part III. Multiple Regression Analysis  
```{r}
multi_fitBaseball = lm(RS ~ BA + SLG + OBP, data = baseballData)
summary(multi_fitBaseball)
plot(multi_fitBaseball)
```
  
For the model RS ∼ BA + SLG + OBP, the intercept is -806.08, and the coefficient for BA is -134.90 with significance p = 0.236  the coefficient for SLG is 1533.88 with significance p < 2e-16 And the coefficient for OBP is 2900.94 with significance p < 2e-16. The second plot above is the QQ plot of the residuals. We can verify that the residuals are not skewly distributed and that the model is reasonable. The fitting results is not consistent with that in Part II, especially for the fitted coefficient of BA. (coeff = 5864.84 vs -134.90, one is a big positive, and the other is negative). We find that BA is not significant and SLG and OBP is significant

```{r}
multi_fitBaseball2 = lm(RS ~ BA + SLG, data = baseballData)
summary(multi_fitBaseball2)
plot(multi_fitBaseball2)
```
  
For the model RS ∼ BA + SLG , the intercept is -551.08, and the coefficient for BA is 1904.66 with significance p < 2e-16  the coefficient for SLG is 1943.77 with significance p < 2e-16. The second plot above is the QQ plot of the residuals. We can verify that the residuals are not skewly distributed and that the model is reasonable. The fitting results is more consistent with that in Part II, especially for the fitted coefficient of BA. (coeff = 5864.84 vs 1904.66 vs -134.90 for model RS ∼ BA + SLG + OBP). Both of these variables are significant.

Comparing the Rsquared for both of these models: for model RS ∼ BA + SLG + OBP, Rsquared = 0.9249, and for model RS ∼ BA + SLG, Rsquared = 0.8711. Therefore the model that we prefer is RS ∼ BA + SLG + OBP since the Rsquared is greater.  


So in the question they want RS ∼ BA + SLG. However we want to remove BA so here is an extra regression for model RS ~ SLG + OBP
```{r}
multi_fitBaseball3 = lm(RS ~ SLG + OBP, data = baseballData)
summary(multi_fitBaseball3)
plot(multi_fitBaseball3)
```
  
For the model RS ∼ SLG + OBP , the intercept is -811.66, and the coefficient for SLG is 1517.58 with significance p < 2e-16  the coefficient for OBP is 2830.70 with significance p < 2e-16. The second plot above is the QQ plot of the residuals. We can verify that the residuals are not skewly distributed and that the model is reasonable. Both of these variables are significant.

Comparing the Rsquared for models: for model RS ∼ BA + SLG + OBP, Rsquared = 0.9249, and for model RS ∼ SLG + OBP, Rsquared = 0.9248. Both Rsquared is roughly the same so we can consider dropping BA and just have SLG and OBP in our model.

## 4. Part IV.  
```{r}
head(baseballData)
```

```{r}
RD <- baseballData$RS - baseballData$RA
baseballData2 <- baseballData
baseballData2$RD=RD
baseballData2 <- baseballData2[baseballData2$Year < 2002, ]
head(baseballData2)
```
```{r}
modelw_rd = lm(W ~ RD, data = baseballData2)
summary(modelw_rd)
```

```{r}
modelrs = lm(RS ~ OBP + SLG, data = baseballData2)
summary(modelrs)
```

```{r}
modelra = lm(RA ~ OOBP + OSLG, data = baseballData2)
summary(modelra)
```

in 2002 OBP = .349, SLG = .430, OOBP = .307 and OSLG = .373 for Oakland Athletics.
we have that RA = -837.38 +2913.60xOOBP + 1514.29xOSLG
RS = -804.63 + 2737.77xOBP + 1584.91xSLG
W = 80.881375 + 0.105766xRD
```{r}
OBP = .349
SLG = .430
OOBP = .307
OSLG = .373
RA = -837.38 +2913.60*OOBP + 1514.29*OSLG
RS = -804.63 + 2737.77*OBP + 1584.91*SLG
RD = RS-RA
W = 80.881375 + 0.105766*RD
W

```
From the three models, we can predict that Oakland would win 103 games in 2002. By looking at our dataset, we see that Oakland did win 103 games in 2002
```{r}
oakland <- baseballData
oakland[oakland$Year == 2002 & oakland$Team == 'OAK', ]
```

